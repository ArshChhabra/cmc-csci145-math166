\documentclass[10pt]{article}

\usepackage[margin=1in]{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage[shortlabels]{enumitem}

\usepackage{hyperref}
\hypersetup{
  colorlinks   = true, %Colours links instead of ugly boxes
  urlcolor     = black, %Colour for external hyperlinks
  linkcolor    = blue, %Colour of internal links
  citecolor    = blue  %Colour of citations
}

\usepackage{colortbl}
\usepackage{array}
\newcolumntype{L}[1]{>{\raggedright\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{C}[1]{>{\centering\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{R}[1]{>{\raggedleft\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}

\usepackage{tikz}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\theoremstyle{definition}
\newtheorem{theorem}{Theorem}
\newtheorem{problem}{Problem}
\newcommand{\R}{\mathbb R}
\DeclareMathOperator*{\E}{\mathbb E}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

\newcommand{\trans}[1]{{#1}^{T}}
\newcommand{\loss}{\ell}
\newcommand{\w}{\mathbf w}
\newcommand{\wstar}{\w^*}
\newcommand{\mle}[1]{\hat{#1}_{\textit{mle}}}
\newcommand{\map}[1]{\hat{#1}_{\textit{map}}}
\newcommand{\normal}{\mathcal{N}}
\newcommand{\x}{\mathbf x}
\newcommand{\y}{\mathbf y}
\newcommand{\ltwo}[1]{\lVert {#1} \rVert}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}


\begin{center}
    {
\Large
Notes: bias-variance tradeoff
}

    \vspace{0.1in}
CSCI145/MATH166, Mike Izbicki

    \vspace{0.1in}
\end{center}

We generate data according to the process
\begin{equation}
    t \sim f(\x) + \epsilon
\end{equation}
where the $\sim$ symbol should be read as ``sampled from'' or ``has distribution.''
The $f$ function is unknown, and our goal is to estimate it from a sample $D=\{(t_1,\x_1),...,(t_n,\x_n)\}$ using a parametric family of functions $y$.
This family is often defined to be linear
\begin{equation}
    y(\x;\w) = \trans\x\w
    ,
\end{equation}
but it does not have to be.
Our only assumption is that there exists some value of $\w$ such that $f(\cdot) = y(\cdot;\w)$.
(This is sometimes called the \textbf{realizability assumption}.)

We measure the quality of our estimator $y$ using the squared error,
which is defined as
\begin{equation}
    \text{squared error}=(t-y(\x;D))^2
\end{equation}
where $y(\x;D) = y(\x;\hat\w)$ and $\hat\w$ is the parameter estimate of $\w$ on dataset $D$.
The squared error of an estimator can be decomposed into three terms:
\begin{equation}
    \text{squared error} = \text{bias}^2 + \text{variance} + \text{noise}
    ,
\end{equation}
where each term is defined to be
\renewcommand{\triangleq}{=}
\begin{align}
    \text{bias}     %&\triangleq \E_D y(\x;D) - \E_t[t|\x] \\[10pt]
                    &\triangleq \left|\E_D y(\x;D) - f(x)\right|       \\[10pt]
    \text{variance} &\triangleq \E_D \bigg( y(\x;D) - \E_Dy(\x;D) \bigg)^2 
                    = \E_D y(\x;D)^2 - (\E_D y(\x;D))^2 \\[12pt]
    \text{noise}    %&\triangleq (t-\E_t[t|\x])^2 \\[10pt]
                    &\triangleq (t-f(x))^2 %\\[10pt]
                    = \epsilon^2
\end{align}
The definitions above are (slightly) different than the definitions in Bishop (Eq. 3.41-3.44).
Bishop takes the expectation of everything with respect to $\x$ and $t$ (which I haven't done above) to get the \emph{mean} squared error.

\begin{theorem}[informal]
    For any ``reasonable'' maximum likelihood problem, 
    %the bias and variance both decay as $O(n^{-1/2})$.
    \begin{align}
        \text{bias} = O(n^{-1}) \qquad \text{and} \qquad
        \text{variance} = \Theta(n^{-1}).
    \end{align}
    The bias may decay at a faster rate, and in particular may be zero.
    But for the variance, this rate is tight.
\end{theorem}

\newpage

\begin{problem}
    If $n$ is large, which is larger: bias or variance?
\end{problem}
\vspace{0.5in}

\begin{problem}
    In the limit as $n\to\infty$, what does the squared error equal?
\end{problem}
\vspace{0.5in}

\subsection*{Visualizing bias and variance}

\begin{center}
\begin{tikzpicture}
    \node at (-1.5in,0in) {low bias};
    \node at (-1.5in,2in) {high bias};
    \node at (0,3in) {high variance};
    \node at (2in,3in) {low variance};
    \node at (0,0) {\textbf. $\wstar$};
    \node at (2in,0) {\textbf. $\wstar$};
    \node at (0,2in) {\textbf. $\wstar$};
    \node at (2in,2in) {\textbf. $\wstar$};
\end{tikzpicture}
\end{center}

\vspace{1in}
\subsection*{How to control the squared error}
\renewcommand{\arraystretch}{2}
\begin{tabular}{!{\color{lightgray}\vrule}l!{\color{lightgray}\vrule}C{1.25in}!{\color{lightgray}\vrule}C{1.25in}!{\color{lightgray}\vrule}C{1.25in}!{\color{lightgray}\vrule}}
    \multicolumn{1}{c}{Action} & \multicolumn{1}{c}{bias} & \multicolumn{1}{c}{variance} & \multicolumn{1}{c}{noise} \\
    \hline
    adding more data &&&\\% & $\downarrow$ & $\downarrow$ & - \\
    \arrayrulecolor{lightgray}
    \hline
    more complex model &&&\\% & $\downarrow$ & $\uparrow$ & - \\
    \hline
    stronger regularization/prior &&&\\% & $\uparrow$ & $\downarrow$ & - \\
    \hline
    adding more features &&&\\% & $\uparrow$ & $\uparrow$ & $\downarrow$ \\ 
    \hline
\end{tabular}

\end{document}


