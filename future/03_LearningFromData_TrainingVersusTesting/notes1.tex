\documentclass[10pt]{exam}

\usepackage[margin=1in]{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{bm}
\usepackage{stmaryrd}

\usepackage{color}
\usepackage{colortbl}
\definecolor{deepblue}{rgb}{0,0,0.5}
\definecolor{deepred}{rgb}{0.6,0,0}
\definecolor{deepgreen}{rgb}{0,0.5,0}
\definecolor{gray}{rgb}{0.7,0.7,0.7}

\usepackage{hyperref}
\hypersetup{
  colorlinks   = true, %Colours links instead of ugly boxes
  urlcolor     = black, %Colour for external hyperlinks
  linkcolor    = blue, %Colour of internal links
  citecolor    = blue  %Colour of citations
}

\usepackage{listings}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\theoremstyle{definition}
\newtheorem{problem}{Problem}
\newtheorem{example}{Example}
\newtheorem{defn}{Definition}
\newtheorem{fact}{Fact}
\newtheorem{refr}{References}
\newtheorem{theorem}{Theorem}
\newcommand{\E}{\mathbb E}
\newcommand{\R}{\mathbb R}
\DeclareMathOperator{\nnz}{nnz}
\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\determinant}{det}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\prob}{\mathbb P}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

\newcommand{\Ein}{E_{\text{in}}}
\newcommand{\Eout}{E_{\text{out}}}
\newcommand{\Etest}{E_{\text{test}}}
\newcommand{\I}{\mathbf I}
\newcommand{\Q}{\mathbf Q}
\newcommand{\p}{\mathbf P}
\newcommand{\pb}{\bar {\p}}
\newcommand{\pbb}{\bar {\pb}}
\newcommand{\pr}{\bm \pi}

\newcommand{\trans}[1]{{#1}^{T}}
\newcommand{\loss}{\ell}
\newcommand{\w}{\mathbf w}
\newcommand{\wstar}{{\w}^{*}}
\newcommand{\x}{\mathbf x}
\newcommand{\y}{\mathbf y}
\newcommand{\lone}[1]{{\lVert {#1} \rVert}_1}
\newcommand{\ltwo}[1]{{\lVert {#1} \rVert}_2}
\newcommand{\lp}[1]{{\lVert {#1} \rVert}_p}
\newcommand{\linf}[1]{{\lVert {#1} \rVert}_\infty}
\newcommand{\lF}[1]{{\lVert {#1} \rVert}_F}

\newcommand{\mH}{m_{\mathcal H}}
\newcommand{\dvc}{{d_{\text{VC}}}}
\newcommand{\HH}[1]{\mathcal H_{\text{#1}}}
\newcommand{\Hbinary}{\HH_{\text{binary}}}
\newcommand{\Haxis}{\HH_{\text{axis}}}
\newcommand{\Hperceptron}{\HH_{\text{perceptron}}}


\newcommand{\ignore}[1]{}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}


\begin{center}
{
\Huge
%Chapter 1/2: The Learning Problem (III)
Chapter 2: Training vs Testing
}
\end{center}

\begin{center}
%\includegraphics[height=3in]{scooby}
%~~~~~~~~~~
%\includegraphics[height=3in]{ml}
\end{center}

\section*{Motivation}

The Finite Hypothesis Class Generalization (FHCG) Theorem from the previous lecture notes gives us the bound
\begin{equation}
    \Eout \le \Ein + O\bigg(\sqrt{\frac{\log M - \log\delta}{N}}\bigg)
    .
\end{equation}
This bound is not useful for infinite hypothesis classes.
The goal of this chapter is to replace the dependence on $M$ above with a dependence on the ``VC dimension'' ($\dvc$)
\begin{equation}
    \Eout \le \Ein + O\bigg(\sqrt{\frac{\dvc - \log\delta}{N}}\bigg)
    .
\end{equation}
We will not prove this result.
But in order to use this result in practice,
we must define the VC dimension and be able to calculate the VC dimension of our hypothesis classes.

\newpage
\section*{Section 2.1.1 Effective Number of Hypotheses}

\begin{defn}
    Let $\x_1,...,\x_N \in \mathcal X$.
    The \emph{dichotomies} generated by a hypothesis class $\mathcal H$ on these points are defined by
    \begin{equation}
        \mathcal H(\x_1, ..., \x_N) = \bigg\{ \big(h(\x_1), ..., h(\x_N)\big) : h \in \mathcal H \bigg\}
    \end{equation}
\end{defn}

\begin{example}
    Consider the dataset of 4 points defined by
    \begin{align*}
    \x_1 = (+1,+1) \\
    \x_2 = (-1,+1) \\
    \x_3 = (+1,-1) \\
    \x_4 = (-1,-1)
    \end{align*}
    and the dataset of 4 points defined by
    \begin{align*}
    \x'_1 = (+1,+1) \\
    \x'_2 = (-1,-1) \\
    \x'_3 = (+2,+2) \\
    \x'_4 = (-2,-2)
    \end{align*}
    What are the dichotomies generated by $\HH{axis}$ hypothesis class on these two datasets?
    Recall that
    \begin{equation*}
    \HH{axis} = \bigg\{ \x \mapsto \sign(x_i) : i \in [d] \bigg\}.
    \end{equation*}
\end{example}


%\begin{problem}
    %You should be able to calculate the dichotomies for any dataset / hypothesis class combination.
    %To practice this, compute the dichotomies for the other finite hypothesis classes from the previous notes.
%\end{problem}

\newpage
\begin{defn}
    The \emph{growth function} for a hypothesis class $\mathcal H$ is defined to be
    \begin{equation}
        m_{\mathcal H}(N) = \max_{\x_1,...,\x_N\in\mathcal X} \big| \mathcal H(\x_1, ..., \x_N) \big|.
    \end{equation}
\end{defn}
\begin{fact}
    For all datasets and all hypothesis classes,
    %\begin{equation}
        $
        \mH(N) \le 2^N.
        $
    %\end{equation}
\end{fact}
%\vspace{2in}

\begin{defn}
    We say that a hypothesis class $\mathcal H$ can \emph{shatter} a dataset $\x_1, ..., \x_N$ if any of the following equivalent statements are true:
    \begin{enumerate}
        \item $\mathcal H$ is capable of generating all possible dichotomies of $\x_1, ..., \x_N$.
        \item $\mathcal H(\x_1, ..., \x_N) = \{-1, +1\}^N$.
        \item $\left|\mathcal H(\x_1, ..., \x_N)\right| = 2^N$.
    \end{enumerate}
    %If $\mathcal H$ is capable of generating all possible dichotomies on $\x_1, ..., \x_N$ (i.e.\ $\mathcal H(\x_1, ..., \x_N) = \{-1, +1\}^N$), then we say that $\mathcal H$ can \emph{shatter} $\x_1, ..., \x_N$.
\end{defn}
\begin{defn}
    If no data set of size $k$ can be shattered by $\mathcal H$, then $k$ is said to be a \emph{break point} for $\mathcal H$.
\end{defn}

\begin{example}
    [Example 2.1, page 43]
    Let $\mathcal H$ be the perceptron hypothesis class in 2 dimensions.
    What is $\mH(3)$ and $\mH(4)$?
\end{example}

\vspace{6in}
\begin{problem}
    Example 2.2 in the textbook (43-45) contains many more examples of computing the growth function.
    You should work through and understand all of these examples.
\end{problem}
\newpage

%\begin{fact}
    %The following two statements are equivalent:
    %\begin{enumerate}
        %\item $\mH(N) = 2^N$.
        %\item $\mathcal H$ shatters some dataset of size $N$.
    %\end{enumerate}
%\end{fact}

%\begin{problem}(Example 2.1)
    %If $\mathcal X=\R^2$ and $\mathcal H$ is the perceptron,
    %then what are $\mH(3)$ and $\mH(4)$?
%\end{problem}

%\newpage

\section*{Section 2.1.2: Bounding the Growth Function}

\begin{theorem}
    If $\mH(k) < 2^k$ for some value $k$, then
    \begin{equation}
        \mH(N)
        \le \sum_{i=0}^{k-1} {N \choose k}
        =
        O(N^{k-1})
        .
    \end{equation}
    This implies that, $\mH$ grows exponentially before its first breakpoint,
    and polynomially thereafter.
\end{theorem}

\newpage
\section*{Section 2.1.3 / 2.1.4: The VC Dimension}
%\section*{Section 2.1.4: The VC Generalization Bound}

\begin{defn}
    The Vapnik-Chervonenkis dimension (VC dimension) of a hypothesis set $\mathcal H$, denoted by $\dvc(\mathcal H)$ or simply $\dvc$, is the largest value of $N$ for which $\mH(N) = 2^N$.
    If $\mH(N) = 2^N$ for all $N$, then $\dvc = \infty$.
\end{defn}

%\begin{fact}
    %If $\mathcal H$ can shatter a set of size $N$, then $\dvc(\mathcal H) \ge N$.
%\end{fact}
%
%\begin{fact}
    %For any hypothesis class $\mathcal H$, the value $\dvc(\mathcal H) + 1$ is a break point for $\mathcal H$.
%\end{fact}

\begin{fact}[Equation 2.9/2.10, page 50]
    \label{fact:2.10}
    For all hypothesis classes $\mathcal H$, we have that
    \begin{equation}
        \label{eq:mHdvc}
        \mH(N) \le N^\dvc + 1
    \end{equation}
\end{fact}

\vspace{3in}
\begin{theorem}[VC generalization bound]
    For any tolerance $\delta>0$, we have that with probability at least $1-\delta,$
    \begin{equation}
        \Eout \le \Ein + \sqrt{\frac8N \log\frac{4\mH(2N)}{\delta}}.
    \end{equation}
    Substituting the bound from Fact \ref{fact:2.10} above,
    we get that
    \begin{align}
        \Eout 
        \le \Ein + \sqrt{\frac8N \log\frac{4(2N)^\dvc + 1}{\delta}} 
        = O\left(\sqrt{\frac{\dvc\log N - \log\delta}{N}}\right).
    \end{align}

\end{theorem}

\newpage
\begin{problem}
    What is the VC dimension of the perceptron hypothesis class?
\end{problem}

\newpage
\begin{problem}
    You are a bank using the perceptron to learn a formula for whether or not to issue a loan.
    \begin{enumerate}
        \item
            You have successfully learned a model on the dataset $(\x_1,y_1), ..., (\x_N,y_N)$ where each $\x_i$ has $d$ features.
            Unfortunately, the training error of the model is too high.
            Management has allocated money to create a new dataset.
            Your choices are to either spend that money to add new features to the existing dataset,
            or to add more data points that all have the same features.
            According to VC theory, which action makes the most sense?

            \vspace{4in}
        \item
            You decided to augment the dataset so that it now has $2d$ features instead of only $d$ features.
            Now the generalization error is too high.
            According to VC theory, how many more data points will you need in order to achieve the same generalization error that you had before?
    \end{enumerate}
\end{problem}

%\begin{problem}
    %What is the VC dimension of the following hypothesis classes?
%
    %\begin{equation}
        %\HH{positive\_rays} = \bigg\{ x \mapsto \sign(x-a) : a \in \R \bigg\}
    %\end{equation}
%
%
    %\begin{equation}
        %\HH{positive\_intervals} = \bigg\{ x \mapsto 
    %\end{equation}
%\end{problem}

\end{document}



