\documentclass[10pt]{article}

\usepackage[margin=1in]{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{bm}
\usepackage{wrapfig}

\usepackage{color}
\usepackage{colortbl}
\definecolor{deepblue}{rgb}{0,0,0.5}
\definecolor{deepred}{rgb}{0.6,0,0}
\definecolor{deepgreen}{rgb}{0,0.5,0}
\definecolor{gray}{rgb}{0.7,0.7,0.7}

\usepackage{hyperref}
\hypersetup{
  colorlinks   = true, %Colours links instead of ugly boxes
  urlcolor     = black, %Colour for external hyperlinks
  linkcolor    = blue, %Colour of internal links
  citecolor    = blue  %Colour of citations
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\theoremstyle{definition}
\newtheorem{problem}{Problem}
\newtheorem{defn}{Definition}
\newtheorem{refr}{References}
\newcommand{\E}{\mathbb E}
\newcommand{\R}{\mathbb R}
\DeclareMathOperator{\nnz}{nnz}
\DeclareMathOperator{\determinant}{det}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

\newcommand{\I}{\mathbf I}
\newcommand{\Q}{\mathbf Q}
\newcommand{\p}{\mathbf P}
\newcommand{\pb}{\bar {\p}}
\newcommand{\pbb}{\bar {\pb}}
\newcommand{\pr}{\bm \pi}

\newcommand{\trans}[1]{{#1}^{T}}
\newcommand{\loss}{\ell}
\newcommand{\w}{\mathbf w}
\newcommand{\x}{\mathbf x}
\newcommand{\y}{\mathbf y}
\newcommand{\lone}[1]{{\lVert {#1} \rVert}_1}
\newcommand{\ltwo}[1]{{\lVert {#1} \rVert}_2}
\newcommand{\lp}[1]{{\lVert {#1} \rVert}_p}
\newcommand{\linf}[1]{{\lVert {#1} \rVert}_\infty}
\newcommand{\lF}[1]{{\lVert {#1} \rVert}_F}

\newcommand{\ignore}[1]{}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}


\begin{center}
{
\Huge
Notes: Pagerank II
}


%\vspace{0.15in}
%Due: Sunday, 6 Sep 2020 at midnight
\end{center}

\begin{center}
%\includegraphics[width=\textwidth]{dilbert}
\end{center}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{wrapfigure}{r}{0.4\textwidth}
    \vspace{-0.6in}
\includegraphics[width=0.9\linewidth]{trivial}
\end{wrapfigure}

\section{Conceptual Understanding}

The ``trivial'' problems in the previous notes packet were designed to help you understand ``what'' the definitions say.
The following problems are designed to help you understand ``why'' the definition of pagerank is defined the way it is.
We do this by exploring the strengths and weaknesses both of pagerank and other possible methods of ranking nodes.

The problems below are ``less trivial'' than the problems in the previous notes packet,
but are still ``fairly trivial''.


%\begin{problem}
%\end{problem}
%
%\newpage
\begin{problem}
    Recall that the purpose of the pagerank vector $\pr$ is to provide a ranking of of how important a node is.
    There are many alternative ways to provide such a ranking.
    One simple alternative is to rank nodes by their in-degree.
    For ``typical'' graphs, the in-degree ranking and the pagerank ranking will be similar,
    but there are graphs for which the two rankings can be arbitrarily different from each other.

    Draw a graph such that the top ranked node according to pagerank is the bottom ranked node according to in-degree.
\end{problem}

\newpage
\begin{problem}
    Either prove or give a counterexample to the following claims.

    \begin{enumerate}
        \item
            The node with the largest out-degree can never have the highest pagerank.
            \newpage
        \item
            Two nodes in a graph cannot have the same pagerank value.
            \newpage
        \item
            Assume that all nodes in the graph have an in-degree of 2.
            Then all nodes will have the same pagerank.
            \newpage
    \end{enumerate}
\end{problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\clearpage
%\begin{problem}
    %The beginning of Section 5 shows the following equivalent definitions for the pagerank vector $\pr$:
    %\begin{equation}
        %\trans \pr \pbb = \trans \pr
        %\qquad
        %\text{and}
        %\qquad
        %\trans \pr (\I - \pbb) = \trans{\bm 0}
        %.
    %\end{equation}
    %It should be obvious why these definitions of $\pr$ are equivalent. 
    %Less obvious (and not shown in the paper) is that
    %the following definition is also equivalent.
    %Prove this equivalence.
%
    %\begin{equation}
        %\pr = \argmax_{\w \in \R^d, \ltwo{\w} \le 1} \ltwo{\trans \w \pbb}
    %\end{equation}
%\end{problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage
\section{Runtimes}

\begin{problem}
    \label{prob:sparse}
    In this question you will calculate the runtime of the power method for computing pagerank.
    Assume that $P$ is a sparse matrix and that $\pr$ is dense.

    \begin{enumerate}
        \item
            Equation 5.1 shows the power method iteration for solving for $\pr$.
            It is reproduced below
            \begin{equation}
                \x^{(k)T}
                =
                \alpha \x^{(k-1)T} \p + \big(\alpha \x^{(k-1)T} \mathbf a + (1-\alpha)\big) \mathbf v^T
                .
                \label{eq:xk}
            \end{equation}
            What is the runtime of computing $\x^{(k)}$ from $\x^{(k-1)}$? % using Equation \eqref{eq:xk} if $\pr$ is stored as a sparse matrix in COO format?
            \vspace{4in}

        \item 
            \label{item:2}
            Given only $\x^{(0)}$, what is the runtime of computing $\x^{(K)}$ by iterating Equation $\eqref{eq:xk}$ $K$ times?
            \vspace{4in}

        \item
            \label{item:3}
            When computing pagerank,
            we typically do not know the final number of iterations $K$ in advance.
            Instead, we continue our computation until the following condition is met:
            \begin{equation}
                \ltwo { \x^{(k)} - \x^{(k-1)} } \le \epsilon,
            \end{equation}
            where $\epsilon$ is a ``small'' number that controls how accurate we want our solution to be.
            The expression $\ltwo { \x^{(k)} - \x^{(k-1)} }$ is often called the \emph{residual}.

            Compute a formula for the number of iterations $K$ required to achieve a residual less than $\epsilon$.

            HINT:
            See the discussion on page 346.
            \vspace{4in}

        \item
            Substitute your answer for part \ref{item:3} into your answer for part \ref{item:2} to get a formula for the overall runtime in terms of the final desired residual $\epsilon$.
            \vspace{4in}

        %\item
            %We say that an algorithm for computing the pagerank \emph{converges} if in the limit as the number of iterations goes to infinity, the algorithm returns the correct pagerank vector.
            %Have we shown that the power method converges?
            %Or are there conditions in which is will \emph{diverge} (i.e.\ not converge)?
            %\vspace{2in}

            \newpage
    \end{enumerate}
\end{problem}
\newpage
\begin{problem}
    \label{prob:dense}
    Repeat Problem \ref{prob:sparse} assuming $P$ is stored as a dense matrix instead of a sparse matrix.
\end{problem}

\newpage
\begin{problem}
    How do the results from problems \ref{prob:sparse} and \ref{prob:dense} above compare to the LAPACK runtimes for computing the top-eigenvalue of a matrix?
\end{problem}

\newpage
\section{Implementation Details}
\begin{problem}
        Why does it never make sense to store $\pr$ as a sparse vector?
\end{problem}

\newpage
\begin{problem}
            Why is the following inequality ``almost always'' true:
            \begin{equation}
                \label{eq:comp:k}
                \ltwo{\x^{(k)}} < \ltwo{\x^{(k-1)}}
                .
            \end{equation}
            \vspace{4in}

            Based on Inequality \eqref{eq:comp:k} above,
            how should we adjust our implementation of the power method to ensure numerical stability?
\end{problem}

%\begin{problem}
        %%\item
            %Now assume that $\pbb$ is stored as a dense matrix.
            %Repeat the calculations for the runtime of the power method in terms of the desired accuracy $\epsilon$.
            %\vspace{4in}
%\end{problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\ignore{
\newpage
\section{Alternative Algorithms}
\begin{problem}
    There are many alternative algorithms for computing pagerank vectors.
    In this problem, we will investigate an algorithm that I call the \emph{exponentially accelerated power method},
    although it does not have a commonly accepted name.
    This is a divide and conquer algorithm that can achieve the same accuracy $\epsilon$ as the power method with only a logarithmic number of iterations.

    The estimated pagerank vector is given by
    \begin{equation}
        \label{eq:exp:y}
        \y^{(K)} = \x^{(0)} \Q_K
        ,
    \end{equation}
    where
    \begin{equation}
        \Q_k = 
        \begin{cases}
            \pbb & \text{if}~k=0 \\
            \Q_{k-1} \Q_{k-1} & \text{otherwise} \\
        \end{cases}
        .
    \end{equation}
    In the standard power method, the matrix $\pbb$ is not stored explicitly,
    but is calculated from the $\p$ matrix.
    In this problem, you can assume for simplicity that the $\pbb$ matrix is stored explicitly as a dense matrix,
    and that $\Q_k$ is also stored as a dense matrix.

    \begin{enumerate}
        \item
            Show that $\y^{(K)} = \x^{(2^K)}$.
            This equivalence is why the algorithm is ``exponentially accelerated.''

            HINT: 
            Use induction to show that $Q_{K} = \pbb^{2^{K}}$.
            The result follows by combining this fact with \eqref{eq:exp:y} and Equation (5.1) in the paper.
            \vspace{3in}

        \newpage
        \item
            What is the runtime of calculating $\Q_k$ given $\Q_{k-1}$? 
            \vspace{4in}

        \item 
            What is the runtime of computing $\y^{(K)}$ in terms of $K$?
            \vspace{3in}

        \newpage
        \item
            As with the standard power method, we do not know the total number of iterations of the exponential power method in advance.
            Instead, we iterate until
            \begin{equation}
                \label{eq:exp:eps}
                \ltwo{\y^{(K)}-\y^{(K-1)}} \le \epsilon,
            \end{equation}
            where $\epsilon$ is a predetermined small constant value.
            Bounding the number of iterations $K$ required to satisfy \eqref{eq:exp:eps} is a bit more technical than in the previous problem.
            You do not have to compute a bound on $K$ yourself,
            and may instead assume that 
            \begin{equation}
                \label{eq:exp:2}
                K = O\bigg( \log \frac{\log \epsilon}{\log \alpha} \bigg)
            \end{equation}
            satisfies \eqref{eq:exp:eps}.
            Notice that this number of iterations is logrithmic compared to the number of iterations in the standard power method,
            and this is where the name exponentially accelerated comes from.

            %%You do not need to understand the technical details of the proof of \eqref{eq:exp:2},
            %%but it is reproduced below for the curious.
            %\begin{align}
                %\ltwo{\y^{(k)} - \y^{(k-1)}}
                %&= \ltwo{\x^{(2^k)} - \x^{2^{k-1}}} \\
                %&= \ltwo{\sum_{i=2^{k-1}}^{2^k - 1} (\x^{(i+1)} - \x^{(i)})} \\
                %&\le \sum_{i=2^{k-1}}^{2^k - 1} \ltwo{(\x^{(i+1)} - \x^{(i)})} \\
                %&\le \sum_{i=2^{k-1}}^{2^k - 1} \alpha^i \\
                %&\le 2^{k-1} \alpha^{2^{k-1}} \\
                %%&\le \alpha^{2^{k-2}}
                %&= O\bigg(\alpha^{2^{k-2}} \bigg)
                %\label{eq:exp:align}
            %\end{align}
            %Next, we set the right hand side of \eqref{eq:exp:align} less than $\epsilon$,
            %and solve for $k$.
            %\begin{align}
                %\alpha^{2^{k-2}} &\le \epsilon \\
                %{2^{k-2}}\log \alpha &\le \log \epsilon \\
                %{2^{k-2}} &\ge \frac{\log \epsilon}{\log \alpha} \\
                %k-2 & \ge \log_2\frac{\log \epsilon}{\log \alpha} \\
                %k & \ge 2 + \log_2\frac{\log \epsilon}{\log \alpha} \\
                %k & = O\bigg(\log \frac{\log\epsilon}{\log \alpha} \bigg) 
            %\end{align}

        %\item
            What is the runtime of computing $\y^{(K)}$ in terms of $\epsilon$?
            \vspace{4.5in}

        \newpage
        \item
            Under what conditions is the exponentially accelerated power method faster than the standard power method?
            \vspace{4in}

        \item
            Under what conditions is it slower?
            \vspace{3in}

        \newpage
        \item
            Why does it not make sense to use the exponentially accelerated power method to compute the pagerank vector?
            %What bad thing would happen if $\p$ was stored as a sparse matrix and $\pbb$ was calculated from $\p$ as in the standard power method?
            \vspace{4in}
%
    \end{enumerate}
\end{problem}
}

\end{document}


