\documentclass[10pt]{article}

\usepackage[margin=1in]{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{bm}

\usepackage{color}
\usepackage{colortbl}
\definecolor{deepblue}{rgb}{0,0,0.5}
\definecolor{deepred}{rgb}{0.6,0,0}
\definecolor{deepgreen}{rgb}{0,0.5,0}
\definecolor{gray}{rgb}{0.7,0.7,0.7}

\usepackage{hyperref}
\hypersetup{
  colorlinks   = true, %Colours links instead of ugly boxes
  urlcolor     = black, %Colour for external hyperlinks
  linkcolor    = blue, %Colour of internal links
  citecolor    = blue  %Colour of citations
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\theoremstyle{definition}
\newtheorem{problem}{Problem}
\newtheorem{defn}{Definition}
\newtheorem{refr}{References}
\newtheorem{note}{Note}
\newcommand{\E}{\mathbb E}
\newcommand{\R}{\mathbb R}
\DeclareMathOperator{\nnz}{nnz}
\DeclareMathOperator{\determinant}{det}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

\newcommand{\I}{\mathbf I}
\newcommand{\Q}{\mathbf Q}
\newcommand{\p}{\mathbf P}
\newcommand{\pb}{\bar {\p}}
\newcommand{\pbb}{\bar {\pb}}
\newcommand{\pr}{\bm \pi}

\newcommand{\trans}[1]{{#1}^{T}}
\newcommand{\loss}{\ell}
\newcommand{\w}{\mathbf w}
\newcommand{\x}{\mathbf x}
\newcommand{\y}{\mathbf y}
\newcommand{\lone}[1]{{\lVert {#1} \rVert}_1}
\newcommand{\ltwo}[1]{{\lVert {#1} \rVert}_2}
\newcommand{\lp}[1]{{\lVert {#1} \rVert}_p}
\newcommand{\linf}[1]{{\lVert {#1} \rVert}_\infty}
\newcommand{\lF}[1]{{\lVert {#1} \rVert}_F}

\newcommand{\ignore}[1]{}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}


\begin{center}
{
\Huge
Notes: Pagerank I
}

%\vspace{0.15in}
%Due: Sunday, 6 Sep 2020 at midnight
\end{center}

\begin{center}
\includegraphics[width=\textwidth]{dilbert}
\end{center}

\section{Background}

Pagerank is one of the most popular data mining techniques.
It was made famous by the founders of Google (Larry Page and Sergey Brin),
who used pagerank to improve the quality of results returned by search engines.
Today, pagerank is also used in most fields that involve data.
For example, it has been used to filter spam from social media,
predict the behavior of drugs without clinical trials,
detect intrusions into computer networks,
and discover bugs in software.
Search engines have significantly since Google first introduced pagerank over 20 years ago,
but pagerank remains an integral part of how search engines work.

This sequence of notes covers some basic algorithms for computing the pagerank.
I emphasize basic because there are literally hundreds of different algorithms that have been developed,
and researchers are still developing new algorithms.
These new, more advanced algorithms use concepts like distributed computing or more advanced math to improve their runtime.
All of these algorithms return the same basic result---
the top eigenvalue of a special graph called the \emph{web graph}.

We will see how various design choices lead to a trade-off in computational accuracy versus speed.
We will also explore an algorithm design pattern called \emph{divide and conquer} for making algorithms faster.
This will prepare you for the next portion of this course based off of the \emph{Learning from Data} textbook,
which will combine the concepts with statistics.

\begin{refr}
    (recommended)
    Matt Cutts was formerly the head of Google's web spam team,
    and now runs the United States Digital Service (a recently created branch of the US government).
    Watch his video on ``How Google Search Works'', which discusses the importance of pagerank at a very high level.
    \begin{quote}
    \url{https://www.youtube.com/watch?v=KyCYyoGusqs}
    \end{quote}
\end{refr}

\begin{refr}
    Our primary text for this week is \emph{Deeper Inside Pagerank} by Langville and Meyers.
    It is available on the github repo or at
    \begin{quote}
    \url{https://galton.uchicago.edu/~lekheng/meetings/mathofranking/ref/langville.pdf}
    \end{quote}
    You will be responsible for sections 1, 2, 3, 5.1, 6.1, 6.2.
\end{refr}

%\begin{problem}
    %(optional)
    %Pagerank has many applications outside of web search,
    %and this problem gives you some additional reading that explores these problems at a high level.
%
    %Read the case study on Twitter's WTF system.
    %And yes, that's it's real name.
    %\begin{quote}
    %\url{https://dl.acm.org/doi/10.1145/2488388.2488433}
    %\end{quote}
    %For more applications of pagerank outside of web search, read section 4 of ``Pagerank Beyond the Web.''
    %\begin{quote}
    %\url{https://arxiv.org/abs/1407.5107}
    %\end{quote}
%\end{problem}

\newpage
\section{Definitions}

\begin{note}
    You will have a closed-note quiz on Problem \ref{problem:def} below.
    For the quiz, I will print out these two sheets of paper exactly as they are in the notes,
    and you will have to fill out the definitions.
\end{note}
\begin{problem}
    \label{problem:def}
    Reproduce the definitions from the reading of the following terms below.
    \begin{enumerate}
        \item irreducible matrix
            \vspace{2in}
        \item primitive matrix
            \vspace{2in}
        \item aperiodic markov chain
            \vspace{2in}
        \item $\p$
            \vspace{2in}
        \item $\pb$
            \vspace{2in}
        \item $\pbb$
            \vspace{2in}
        \item $\pr$
            \vspace{2in}
        \item $\mathbf v$ %personalization vector
            \vspace{2in}
        %\item dangling nodes
            %\vspace{2in}
    \end{enumerate}
\end{problem}

\newpage
\begin{problem}
    The reading uses the following terms,
    but does not explicitly define them.
    Use wikipedia to find definitions for these terms.

    You will not be tested on the exact, formal definitions of these terms.
    You just need to be able to use and understand them in context.

It is common for data mining papers to not define common terms.
%(\emph{Deeper Inside Pagerank} is already 50+ pages!)
When I'm reading papers, I'm constantly having to look up definitions on wikipedia for these terms in order to understand the paper.
One of the challenges of wikipedia is that there's a lot of information about each of these terms that may or may not be relevant.
The main challenge of this problem is figuring out how to define these terms so that they help you understand the rest of this paper.
I therefore recommend not looking up these terms in wikipedia until you encounter them in the reading;
then provide definitions below that help you understand the reading.
%There's a lot of information on wikipedia,
%and you need to try to find the most relevant parts.

    \begin{enumerate}
        \item markov chain
            \vspace{2.5in}
        \item stationary vector
            \vspace{2.5in}
        \item stochastic matrix
            \vspace{2.5in}
        \item spectral radius
            \vspace{2.5in}
        \item subdominant eigenvalue
    \end{enumerate}
\end{problem}

\newpage
\section{``Trivial'' Problems}
The trivial problems in this section are designed to help you practice using the definitions above.
%Assuming you fully understand the definitions,
%all of these problems should be ``trivial''.

\begin{note}
Mathematicians define a problem to be \emph{trivial} if the solution follows directly from the definitions without needing any major insights.
Trivial problems can still take a long time to solve, however, because understanding the definitions is hard.
Muggles%
\footnote{In the \emph{Harry Potter} books, \emph{muggles} are people who cannot use magic.
I call non-mathematicians/non-computer scientists muggles because they see the sorts of things we do in this class as ``magic''.}
think that ``trivial'' problems should be ``easy'' to solve,
but that's only the case if you have a really strong understanding of the involved definitions.
It's okay if these trivial problems do not feel easy.
\end{note}

\begin{problem}
    Give an example of:
    \begin{enumerate}
        \item a stochastic matrix
            \vspace{3in}
        \item a non-stochastic matrix
            \vspace{3in}

            ~
            \newpage
        \item an irreducible matrix
            \vspace{4in}
        \item a reducible matrix
            \vspace{4in}
            \newpage
        \item a primitive matrix
            \vspace{4in}
        \item a non-primitive matrix
    \end{enumerate}
\end{problem}

\newpage
\begin{note}
Whenever I'm reading a data mining text,
and I encounter a matrix,
    I always ask myself what sorts of properties the matrix might have.
The purpose of the questions below is to help get you into a similar habit.
\end{note}
\begin{problem}
    Answer the following questions.
    \begin{enumerate}
        \item 
            Is the matrix $\p$ stochastic? irreducible? primitive?
            \vspace{3in}
        \item 
            Is the matrix $\pb$ stochastic? irreducible? primitive?
            \vspace{3in}
        \item 
            Is the matrix $\pbb$ stochastic? irreducible? primitive?
            \vspace{3in}
    \end{enumerate}
\end{problem}

\newpage

\begin{problem}
    Either prove or give a counterexample to the following claims.

    HINT:
    Any claim which is true will have a ``trivial'' proof.
    When looking for counterexamples, use the identity and zero matrices as building-blocks.
    \begin{enumerate}
        \item
            $\rank(\pbb) = 1$.
            \vspace{4.5in}
        \item
            $\rank(\pbb) = n$.
            \vspace{4.5in}

        \item
            Let $\x$ be an eigenvector of $\p$ with eigenvalue $\lambda$.
            Then $\frac 1 2 \x$ is also an eigenvector of $\p$ with eigenvalue $\frac 1 2 \lambda$.
            \vspace{4.5in}

        \item
            The smallest eigenvalue of $\pbb$ is exactly 0.
            \vspace{4.5in}
        \item
            The largest eigenvalue of $\pbb$ is exactly 1.
            \vspace{4.5in}
        \item
            The largest eigenvalue of $\p$ is exactly 1.
            \vspace{4.5in}

        \item
            The largest eigenvector of $\p$ is simple.

            (Recall that a \emph{simple} eigenvalue has multiplicity 1.
            That is, there is exactly 1 eigenvector with the same eigenvalue.)
            \vspace{4.5in}

        \item
            The largest eigenvector of $\pb$ is simple.
            \vspace{4.5in}

        \item
            The largest eigenvector of $\pbb$ is simple.
            \vspace{4.5in}

        %\item
            %The eigenvectors of $\pbb$ are orthogonal.
            %\vspace{4.5in}
%
        %\item
            %The eigenvectors of $\pbb\trans\pbb$ are orthogonal.
            %\vspace{4.5in}
%
        %\item
            %The eigenvectors of $\trans\pbb\pbb$ are orthogonal.
            %\vspace{4.5in}
    \end{enumerate}
\end{problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\ignore{
\newpage
\section{Pagerank}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{problem}
    The purpose of the pagerank vector $\pr$ is to provide a ranking of of how important a node is.
    There are many alternative ways to provide such a ranking.
    One simple alternative is to rank nodes by their in-degree.
    For ``typical'' graphs, the in-degree ranking and the pagerank ranking will be similar,
    but there are graphs for which the two rankings can be arbitrarily different from each other.

    Draw a graph such that the top ranked node according to pagerank is the bottom ranked node according to in-degree.
\end{problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\clearpage
%\begin{problem}
    %The beginning of Section 5 shows the following equivalent definitions for the pagerank vector $\pr$:
    %\begin{equation}
        %\trans \pr \pbb = \trans \pr
        %\qquad
        %\text{and}
        %\qquad
        %\trans \pr (\I - \pbb) = \trans{\bm 0}
        %.
    %\end{equation}
    %It should be obvious why these definitions of $\pr$ are equivalent. 
    %Less obvious (and not shown in the paper) is that
    %the following definition is also equivalent.
    %Prove this equivalence.
%
    %\begin{equation}
        %\pr = \argmax_{\w \in \R^d, \ltwo{\w} \le 1} \ltwo{\trans \w \pbb}
    %\end{equation}
%\end{problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage

\begin{problem}
    In this question you will calculate the runtime of the power method for computing pagerank.
    Assume that $P$ is a sparse matrix and that $\pr$ is dense.

    \begin{enumerate}
        \item
            Equation 5.1 shows the power method iteration for solving for $\pr$.
            It is reproduced below
            \begin{equation}
                \x^{(k)T}
                =
                \alpha \x^{(k-1)T} \p + \big(\alpha \x^{(k-1)T} \mathbf a + (1-\alpha)\big) \mathbf v^T
                .
                \label{eq:xk}
            \end{equation}
            What is the runtime of computing $\x^{(k)}$ from $\x^{(k-1)}$? % using Equation \eqref{eq:xk} if $\pr$ is stored as a sparse matrix in COO format?
            \vspace{4in}

        \item 
            \label{item:2}
            Given only $\x^{(0)}$, what is the runtime of computing $\x^{(K)}$ by iterating Equation $\eqref{eq:xk}$ $K$ times?
            \vspace{4in}

        \item
            \label{item:3}
            When computing pagerank,
            we typically do not know the final number of iterations $K$ in advance.
            Instead, we continue our computation until the following condition is met:
            \begin{equation}
                \ltwo { \x^{(k)} - \x^{(k-1)} } \le \epsilon,
            \end{equation}
            where $\epsilon$ is a ``small'' number that controls how accurate we want our solution to be.
            The expression $\ltwo { \x^{(k)} - \x^{(k-1)} }$ is often called the \emph{residual} of the iteration.

            Compute a formula for the number of iterations $K$ required to achieve a residual less than $\epsilon$.

            HINT:
            See the discussion on page 346.
            \vspace{4in}

        \item
            Substitute your answer for part \ref{item:3} into your answer for part \ref{item:2} to get a formula for the overall runtime in terms of the final desired accuracy $\epsilon$.
            \vspace{4in}

        \item
            Now assume that $\pbb$ is stored as a dense matrix.
            Repeat the calculations for the runtime of the power method in terms of the desired accuracy $\epsilon$.
            \vspace{4in}

        %\item
            %We say that an algorithm for computing the pagerank \emph{converges} if in the limit as the number of iterations goes to infinity, the algorithm returns the correct pagerank vector.
            %Have we shown that the power method converges?
            %Or are there conditions in which is will \emph{diverge} (i.e.\ not converge)?
            %\vspace{2in}

            \newpage
        \item
            Why does it never make sense to store $\pr$ as a sparse vector?
            \vspace{2in}

            \newpage
        \item
            Why is the following inequality ``almost always'' true:
            \begin{equation}
                \label{eq:comp:k}
                \ltwo{\x^{(k)}} < \ltwo{\x^{(k-1)}}
                .
            \end{equation}
            \vspace{4in}
        \item
            Based on Inequality \eqref{eq:comp:k} above,
            how should we adjust our implementation of the power method to ensure numerical stability?

    \end{enumerate}
\end{problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage
\begin{problem}
    There are many alternative algorithms for computing pagerank vectors.
    In this problem, we will investigate an algorithm that I call the \emph{exponentially accelerated power method},
    although it does not have a commonly accepted name.
    This is a divide and conquer algorithm that can achieve the same accuracy $\epsilon$ as the power method with only a logarithmic number of iterations.

    The estimated pagerank vector is given by
    \begin{equation}
        \label{eq:exp:y}
        \y^{(K)} = \x^{(0)} \Q_K
        ,
    \end{equation}
    where
    \begin{equation}
        \Q_k = 
        \begin{cases}
            \pbb & \text{if}~k=0 \\
            \Q_{k-1} \Q_{k-1} & \text{otherwise} \\
        \end{cases}
        .
    \end{equation}
    In the standard power method, the matrix $\pbb$ is not stored explicitly,
    but is calculated from the $\p$ matrix.
    In this problem, you can assume for simplicity that the $\pbb$ matrix is stored explicitly as a dense matrix,
    and that $\Q_k$ is also stored as a dense matrix.

    \begin{enumerate}
        \item
            Show that $\y^{(K)} = \x^{(2^K)}$.
            This equivalence is why the algorithm is ``exponentially accelerated.''

            HINT: 
            Use induction to show that $Q_{K} = \pbb^{2^{K}}$.
            The result follows by combining this fact with \eqref{eq:exp:y} and Equation (5.1) in the paper.
            \vspace{3in}

        \newpage
        \item
            What is the runtime of calculating $\Q_k$ given $\Q_{k-1}$? 
            \vspace{4in}

        \item 
            What is the runtime of computing $\y^{(K)}$ in terms of $K$?
            \vspace{3in}

        \newpage
        \item
            As with the standard power method, we do not know the total number of iterations of the exponential power method in advance.
            Instead, we iterate until
            \begin{equation}
                \label{eq:exp:eps}
                \ltwo{\y^{(K)}-\y^{(K-1)}} \le \epsilon,
            \end{equation}
            where $\epsilon$ is a predetermined small constant value.
            Bounding the number of iterations $K$ required to satisfy \eqref{eq:exp:eps} is quite a bit more technical than in the previous problem.
            You do not have to compute a bound on $K$ yourself,
            and may instead assume that 
            \begin{equation}
                \label{eq:exp:2}
                K = O\bigg( \log \frac{\log \epsilon}{\log \alpha} \bigg)
            \end{equation}
            satisfies \eqref{eq:exp:eps}.
            Notice that this number of iterations is logrithmic compared to the number of iterations in the standard power method,
            and this is where the name exponentially accelerated comes from.

            %%You do not need to understand the technical details of the proof of \eqref{eq:exp:2},
            %%but it is reproduced below for the curious.
            %\begin{align}
                %\ltwo{\y^{(k)} - \y^{(k-1)}}
                %&= \ltwo{\x^{(2^k)} - \x^{2^{k-1}}} \\
                %&= \ltwo{\sum_{i=2^{k-1}}^{2^k - 1} (\x^{(i+1)} - \x^{(i)})} \\
                %&\le \sum_{i=2^{k-1}}^{2^k - 1} \ltwo{(\x^{(i+1)} - \x^{(i)})} \\
                %&\le \sum_{i=2^{k-1}}^{2^k - 1} \alpha^i \\
                %&\le 2^{k-1} \alpha^{2^{k-1}} \\
                %%&\le \alpha^{2^{k-2}}
                %&= O\bigg(\alpha^{2^{k-2}} \bigg)
                %\label{eq:exp:align}
            %\end{align}
            %Next, we set the right hand side of \eqref{eq:exp:align} less than $\epsilon$,
            %and solve for $k$.
            %\begin{align}
                %\alpha^{2^{k-2}} &\le \epsilon \\
                %{2^{k-2}}\log \alpha &\le \log \epsilon \\
                %{2^{k-2}} &\ge \frac{\log \epsilon}{\log \alpha} \\
                %k-2 & \ge \log_2\frac{\log \epsilon}{\log \alpha} \\
                %k & \ge 2 + \log_2\frac{\log \epsilon}{\log \alpha} \\
                %k & = O\bigg(\log \frac{\log\epsilon}{\log \alpha} \bigg) 
            %\end{align}

        %\item
            What is the runtime of computing $\y^{(K)}$ in terms of $\epsilon$?
            \vspace{4.5in}

        \newpage
        \item
            Under what conditions is the exponentially accelerated power method faster than the standard power method?
            \vspace{4in}

        \item
            Under what conditions is it slower?
            \vspace{3in}

        \newpage
        \item
            What bad thing would happen if $\p$ was stored as a sparse matrix and $\pbb$ was calculated from $\p$ as in the standard power method?
            \vspace{4in}

    \end{enumerate}
\end{problem}
}

\end{document}


