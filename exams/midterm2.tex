\documentclass[10pt]{exam}

\usepackage[margin=1in]{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{bm}
\usepackage[normalem]{ulem}
\usepackage{stmaryrd}
\usepackage{booktabs}

\usepackage{color}
\usepackage{colortbl}
\definecolor{deepblue}{rgb}{0,0,0.5}
\definecolor{deepred}{rgb}{0.6,0,0}
\definecolor{deepgreen}{rgb}{0,0.5,0}
\definecolor{gray}{rgb}{0.7,0.7,0.7}

\usepackage{hyperref}
\hypersetup{
  colorlinks   = true, %Colours links instead of ugly boxes
  urlcolor     = black, %Colour for external hyperlinks
  linkcolor    = blue, %Colour of internal links
  citecolor    = blue  %Colour of citations
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newcommand*{\hl}[1]{\colorbox{yellow}{#1}}

\newcommand*{\answerLong}[2]{
    \ifprintanswers{\hl{#1}}
\else{#2}
\fi
}

\newcommand*{\answer}[1]{\answerLong{#1}{~}}

\newcommand*{\TrueFalse}[1]{%
\ifprintanswers
    \ifthenelse{\equal{#1}{T}}{%
        %\hl{\textbf{TRUE}}\hspace*{14pt}False
        \hl{\texttt{True}}\hspace*{20pt}\texttt{False}\hspace*{20pt}\texttt{Open}
    }{
        \ifthenelse{\equal{#1}{F}}{
        %True\hspace*{14pt}\hl{\textbf{FALSE}}
        \texttt{True}\hspace*{20pt}\hl{\texttt{False}}\hspace*{20pt}\texttt{Open}
        }
        {
            \texttt{True}\hspace*{20pt}{\texttt{False}}\hspace*{20pt}\hl{\texttt{Open}}
        }
    }
\else
    \texttt{True}\hspace*{20pt}\texttt{False}\hspace*{20pt}\texttt{Open}
\fi
} 
%% The following code is based on an answer by Gonzalo Medina
%% https://tex.stackexchange.com/a/13106/39194
\newlength\TFlengthA
\newlength\TFlengthB
\settowidth\TFlengthA{\hspace*{1.8in}}
\newcommand\TFQuestion[2]{%
    \setlength\TFlengthB{\linewidth}
    \addtolength\TFlengthB{-\TFlengthA}
    \noindent
    \parbox[t]{\TFlengthA}{\TrueFalse{#1}}\parbox[t]{\TFlengthB}{#2}
    \vspace{0.25in}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\theoremstyle{definition}
\newtheorem{problem}{Problem}
\newtheorem{theorem}{Theorem}
\newtheorem{defn}{Definition}
\newtheorem{refr}{References}
\newcommand{\E}{\mathbb E}
\newcommand{\R}{\mathbb R}
\DeclareMathOperator{\nnz}{nnz}
\DeclareMathOperator{\determinant}{det}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator{\sign}{sign}

\newcommand{\I}{\mathbf I}
\newcommand{\Q}{\mathbf Q}
\newcommand{\p}{\mathbf P}
\newcommand{\pb}{\bar {\p}}
\newcommand{\pbb}{\bar {\pb}}
\newcommand{\pr}{\bm \pi}

\newcommand{\trans}[1]{{#1}^{T}}
\newcommand{\loss}{\ell}
\newcommand{\w}{\mathbf w}
\newcommand{\x}{\mathbf x}
\newcommand{\y}{\mathbf y}
\newcommand{\lone}[1]{{\lVert {#1} \rVert}_1}
\newcommand{\ltwo}[1]{{\lVert {#1} \rVert}_2}
\newcommand{\lp}[1]{{\lVert {#1} \rVert}_p}
\newcommand{\linf}[1]{{\lVert {#1} \rVert}_\infty}
\newcommand{\lF}[1]{{\lVert {#1} \rVert}_F}

\newcommand{\Ein}{E_{\text{in}}}
\newcommand{\Eout}{E_{\text{out}}}
\newcommand{\Etest}{E_{\text{test}}}
\newcommand{\Eapp}{E_{\text{app}}}
\newcommand{\mH}{m_{\mathcal H}}
\newcommand{\dvc}{{d_{\text{VC}}}}
\newcommand{\HH}[1]{\mathcal H_{\text{#1}}}
\newcommand{\Hbinary}{\HH_{\text{binary}}}
\newcommand{\Haxis}{\HH_{\text{axis}}}
\newcommand{\Hperceptron}{\HH_{\text{perceptron}}}
\newcommand{\ignore}[1]{}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\printanswers

\begin{document}


\begin{center}
{
\Huge
    Midterm 2
}
\end{center}

\vspace{0.5in}
\noindent
\textbf{Printed Name:}

\noindent
\rule{\textwidth}{0.1pt}
\vspace{0.25in}

%Format:
%
    %The midterm will be worth 15 points.
    %There will be 10 True / False / Open questions. Each correct answer will be worth 1 point, each incorrect answer will result in a -1 penalty, and each blank answer will result in 0 points.
    %There will be 5 free response questions. These questions will follow the format of the questions in the notes packets.
%
    \noindent
\textbf{Due date:}
\begin{enumerate}
    \item
        The exam is due Sunday 6 Nov at midnight.

    \item
        You may submit it either on sakai electronically or by putting a physical copy under my door.
\end{enumerate}

\vspace{0.15in}
\noindent
\textbf{Rules:}
\begin{enumerate}
    \item
        The exam is untimed.
        You do not have to complete the exam in a single sitting.
        You may pause and restart whenever you'd like.

    \item
    You may use any non-human resources that you like, including notes, books, internet articles, and computers.

    \item
    You are not allowed to discuss the exam in any way with any human until after the due date. This includes:
        \begin{enumerate}
            \item obviously bad behavior like copying answers, 
            \item more banal behavior such as:
                \begin{enumerate}
                    \item telling your friend "Problem 6 was really hard" or 
                    \item asking your friend "Have you completed the exam yet?"
                \end{enumerate}
        \end{enumerate}
        
        Even after you finish the exam, you may not discuss it.

    \item
    If you do have questions about the exam, you should email me the questions rather than posting to github.

\end{enumerate}

\vspace{0.15in}
\noindent
\textbf{Grading:}
\begin{enumerate}
    \item For the True/False/Open questions: Each correct answer will be awarded +1 point, each incorrect answer will result in a -1 point penalty, and each blank answer will result in 0 points.

    \item All other problems are worth 1 point, with no penalty for incorrect answers.

    \item There are 16 points possible on the exam.
        Your final grade entered into sakai will be
        \begin{equation*}
            \min\{15, \text{the number of points earned}\}.
        \end{equation*}

    \item If you find a substantive error on the exam, then I will award you +1 bonus point.
\end{enumerate}

\vspace{0.15in}
\noindent
\textbf{Good luck :)}

\newpage

\begin{problem}
    For each statement below,
    circle \texttt{True} if the statement is known to be true,
    \texttt{False} if the statement is known to be false,
    and \texttt{Open} if the statement is not known to be either true or false.
    Ensure that you pay careful attention to the formal definitions of asymptotic notation in your responses.

\begin{enumerate}
    %\item\TFQuestion{F}{Let $f$ be the true label function.
        %Then it always the case that $\Eout(f) = 0$.}
    %\item\TFQuestion{T}{
        %}

    \item\TFQuestion{O}{For learning problems that are not linearly separable, the fastest possible algorithm for minimizing the 0-1 loss runs in exponential time in the number of feature dimensions $d$.}

    \item\TFQuestion{F}{Let $\mathcal H_1$ and $\mathcal H_2$ be two hypothesis classes satisfying $\dvc(\mathcal H_1) \le \dvc(\mathcal H_2)$.  Then $H_1 \subset H_2$.}

    \item\TFQuestion{T}{Let $\mathcal H$ be the perceptron hypothesis class with the number of feature dimensions $d=8$.  Then $\mH(6)=64$.}

    %\item\TFQuestion{T}{Let $\mathcal H_1$ be the perceptron hypothesis class with the PCA feature map applie
        
    \item\TFQuestion{T}{Let $\mathcal H$ be the perceptron hypothesis class and $\mathcal H_\Phi$ be the perceptron hypothesis class with the PCA kernel applied.
        Furthermore let $g\in\mathcal H$ and $g_\Phi\in\mathcal H_\Phi$ be the empirical risk minimizers.
        Then $\Ein(g) \le \Ein(g_\Phi)$.
        }
        
    \item\TFQuestion{F}{Let $\mathcal H$ be the perceptron hypothesis class and let $\mathcal H_\Phi$ be the perceptron hypothesis class with the decision stump feature map.
        Let $g\in\mathcal H$ and $g_\Phi \in \mathcal H_\Phi$ be the emperical risk minimizers.
        Then VC theory provides a better generalization bound for $g$ than for $g_\Phi$.}
        
    %\item\TFQuestion{T}{You have trained a logistic regression model $g$.  You evaluate it on your training set and test set and you notice that $|\Ein(g)-\Etest(g)|$ is much larger than $\Ein(g)$.  VC theory predicts that applying the PCA feature embedding with a low output dimension will reduce the generalization error.}

    \item\TFQuestion{F}{Define the approximation error of a hypothesis class $\mathcal{H}$ to be 
        \begin{equation}
            \Eapp = \min_{h\in\mathcal H} \Eout(h)
            .
        \end{equation}
        Then applying the random feature embedding with a low output degree will decrease the approximation error.}

    \item\TFQuestion{T}{Let
        $$
    \HH{axis2} = \bigg\{ \x \mapsto \sigma\sign(x_i) : \sigma \in\{+1, -1\}, i \in [d] \bigg\},
        $$
        and
        $$
    \HH{axis} = \bigg\{ \x \mapsto \sign(x_i) : i \in [d] \bigg\},
        $$
        Let $g_{\text{axis2}} \in \HH{axis2}$ and $g_{\text{axis}}\in\HH{axis}$ be the outputs of the TEA algorithm on their respective hypothesis classes.
        Then $\Ein(g_{\text{axis2}}) \le \Ein(g_{\text{axis}})$.
        }

    \item\TFQuestion{F}{
            Let $\mathcal H_1$ be the perceptron hypothesis class with the decision stump feature map applied,
            and $\mathcal H_2$ be the perceptron hypothesis class with the polynomial kernel of degree 3.
            Furthermore, let $g_1\in\mathcal H_1$ and $g_2\in\mathcal H_2$ be the emperical risk minimizers.
            Then VC theory predicts that $|\Eout(g_1)-\Etest(g_1)| \le |\Eout(g_2) - \Etest(g_2)|$ with high probability.
        }

    \item\TFQuestion{T}{
            Define the hypothesis class of positive and negative intervals in 1 dimension to be
            \begin{equation}
                \mathcal H = \bigg\{ x \mapsto \sigma \big\llbracket a \le x \le b \big\rrbracket : a \in \R, b \in \R, \sigma \in \{+1, -1\} \bigg\}.
            \end{equation}
            Then the $\dvc(\HH{}) = 3$.
        }

    %\item\TFQuestion{F}{
            %Let $\mathcal H$ be a hypothesis class.
            %If $\min_{h\in\mathcal H} \Ein(h) = 0$, then the VC dimension of $\mathcal H$ must be finite.
        %}

    \item\TFQuestion{F}{
            There does not exist a break point for the perceptron hypothesis class.
        }
\end{enumerate}
\end{problem}

\newpage
\begin{problem}
Either prove or give a counterexample to the following claim: 
Let $f$ be the true label function.
Then it must be the case that $\Ein(f)=0$.
\end{problem}
\begin{solution}
    False.

    Any data distribution with noise will have $\Ein(f)>0$,
    and $\Ein(f)$ is a measure of that noise.

    Common mistakes:
    \begin{enumerate}
    \item (-1) If you didn't talk about noise/randomness/probability distributions/etc, you couldn't get credit for this problem.
    \item (-1) If you stated that $\Ein(f)=0$ would imply overfitting.
    \end{enumerate}
\end{solution}

\newpage
\begin{problem}
Either prove or give a counterexample to the following claim: 
Every surrogate loss function is convex.
%If $\mathcal H$ be a hypothesis class with $\dvc(\HH{})=4$.  Then $\mathcal H$ is guaranteed to be able to shatter all datasets of size $N=3$.
\end{problem}
\begin{solution}
    False.
    The truncated hinge loss is an example of a non-convex surrogate loss.
\end{solution}

\newpage
\begin{problem}
What is the VC dimension of the following hypothesis class?
\begin{equation}
    \HH{} = \bigg\{ \x \mapsto \sigma\big\llbracket \ltwo{\x} \le \alpha \big\rrbracket : \sigma\in\{+1,-1\}, \alpha \in \R \bigg\}
\end{equation}
\end{problem}
\begin{solution}
    $\dvc(\HH{})=2.$

    Select any dataset with two data points of different norms, and $\HH{}$ will shatter this dataset.
    This shows that $\dvc\ge2$.

    There is no dataset of size 3 that can be shattered by $\HH{}$,
    so $\dvc<3$.
\end{solution}

\newpage
\begin{problem}
    You are a bank using logistic regression to learn a formula for whether or not to issue a loan.
    Your dataset has $N$ data points and $d$ features,
    and you have trained a model $g$ using second order gradient descent so that your optimization error is negligible.
    You evaluate your model on the training and test sets and get values of $\Ein(g)=0.05$ and $\Etest(g)=0.41$.

    Your boss suggests that augmenting the dataset with more features might improve performance.
    Is this a good idea?  Use VC to justify why.
\end{problem}
\begin{solution}
    No.

    Although we can't directly measure the generalization error $|\Ein - \Eout|$,
    we can estimate it with $\Ein-\Etest$ because $\Etest \approx \Eout$.
    This estimated generalization error is large relative to the in-sample error $\Ein$.
    Therefore, reducing the VC dimension will likely improve performance.

    Adding more feature dimensions, however, will result in the VC-dimension increasing, and the generalization error increasing.
    Since the in-sample error is already so small,
    it cannot decrease much as a result of these additional dimensions.
    And so the increase in generalization error is unlikely to be counterbalanced by a decrease in in-sample error.


    Common mistakes:
    \begin{enumerate}
        \item (-1) Not discussing VC theory/generalization error at all.
        \item (-0.5): Stating that changing $d$ will have no effect on $\Etest$ is incorrect.
            It is true that changing $d$ will not have an effect on $\Etest-\Eout$, since this is only determined by the size of the test set.

        \item (-0.5): If you stated that adding dimension increases generalization error, but did not state why that is bad for this problem.
            Several students implied that increasing generalization error is always bad for every problem, and this is incorrect.

        \item (no points deducted) Many students (especially non-CS students) assumed that the word ``performance'' referred to runtime.
            In general, the ``performance'' of an algorithm refers to the quality of the solution foremost and to runtime only secondarily.
            For these machine learning algorithms in particular, ``performance'' refers to the statistical guarantees that they offer about the quality of $\Eout$.
            The generalization error $|\Eout-\Ein|$ is closely related to $\Eout$,
            but it is not the ultimate measure of performance for an algorithm.

            One way to think about this is that we have already run the algorithm to completion above.
            So why should we care about the runtime of something that's already complete?
    \end{enumerate}
\end{solution}

\newpage
\begin{problem}
You are training a logistic regression model with $N=10^{12}$ and $d=10^6$.
Which optimization algorithm do you choose and why?
\end{problem}
\begin{solution}
    SGD is the best option.

    Because the number of data points and feature dimensions is large,
    you are likely to be computationally limited rather than statistically limited.
    In this situation, we want the optimization algorithm that has the fastest time to excess error $\epsilon$ rather than the algorithm with the fastest time to accuracy $\rho$.
    SGD has the fastest time to excess error $\epsilon$.

    You could get full credit for choosing 2SGD if you stated that:
    You are not computationally limited on this problem,
    and so are trying to minimize the optimization error $\rho$,
    and that 2SGD is the fastest time to accuracy $\rho$.

    Common mistakes:
    \begin{enumerate}
        \item (-1) No mention of the tradeoff between being computationally limited or not or discussion about the difference between the time to excess error $\epsilon$ and the time to accuracy $\rho$.
        \item (-1) Lots of people stated that the runtime per iteration of SGD does not depend on $N$ and used this to justify using SGD for large $N$.
            It is true that the time per iteration is independent of $N$, but it's not a statement that we care about since it neglects the number of iterations.
        \item (-1) Discussing feature maps instead of optimization algorithms.

        \item For students who had already missed many points, I graded slightly more generously.
    \end{enumerate}
\end{solution}

\newpage
\begin{problem}
    You work at a car manufacturer and are developing a model to determine whether a part is defective or not.
    You are required by regulators to use support vector machines optimized with 2nd order gradient descent,
    but you are free to select any feature maps that you would like.
    Your training data has many features ($d=10^6$) but only a small number of data points ($N=10^3$).
    Which feature maps does VC theory predict would be a good choice?
\end{problem}
\begin{solution}
    The VC dimension is much larger than the number of training data points, and so the feature map needs to reduce the dimensionality.
    The random feature embeddings, PCA, and decision stumps all achieve this goal, and any one of those answers received full credit.

    Common mistakes:
    \begin{enumerate}
        \item (-0.5) If you did not state why we want to decrease generalization error in this specific case.
    \end{enumerate}
\end{solution}

\end{document}




