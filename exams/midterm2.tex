\documentclass[10pt]{exam}

\usepackage[margin=1in]{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{bm}
\usepackage[normalem]{ulem}
\usepackage{stmaryrd}
\usepackage{booktabs}

\usepackage{color}
\usepackage{colortbl}
\definecolor{deepblue}{rgb}{0,0,0.5}
\definecolor{deepred}{rgb}{0.6,0,0}
\definecolor{deepgreen}{rgb}{0,0.5,0}
\definecolor{gray}{rgb}{0.7,0.7,0.7}

\usepackage{hyperref}
\hypersetup{
  colorlinks   = true, %Colours links instead of ugly boxes
  urlcolor     = black, %Colour for external hyperlinks
  linkcolor    = blue, %Colour of internal links
  citecolor    = blue  %Colour of citations
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newcommand*{\hl}[1]{\colorbox{yellow}{#1}}

\newcommand*{\answerLong}[2]{
    \ifprintanswers{\hl{#1}}
\else{#2}
\fi
}

\newcommand*{\answer}[1]{\answerLong{#1}{~}}

\newcommand*{\TrueFalse}[1]{%
\ifprintanswers
    \ifthenelse{\equal{#1}{T}}{%
        %\hl{\textbf{TRUE}}\hspace*{14pt}False
        \hl{\texttt{True}}\hspace*{20pt}\texttt{False}\hspace*{20pt}\texttt{Open}
    }{
        \ifthenelse{\equal{#1}{F}}{
        %True\hspace*{14pt}\hl{\textbf{FALSE}}
        \texttt{True}\hspace*{20pt}\hl{\texttt{False}}\hspace*{20pt}\texttt{Open}
        }
        {
            \texttt{True}\hspace*{20pt}{\texttt{False}}\hspace*{20pt}\hl{\texttt{Open}}
        }
    }
\else
    \texttt{True}\hspace*{20pt}\texttt{False}\hspace*{20pt}\texttt{Open}
\fi
} 
%% The following code is based on an answer by Gonzalo Medina
%% https://tex.stackexchange.com/a/13106/39194
\newlength\TFlengthA
\newlength\TFlengthB
\settowidth\TFlengthA{\hspace*{1.8in}}
\newcommand\TFQuestion[2]{%
    \setlength\TFlengthB{\linewidth}
    \addtolength\TFlengthB{-\TFlengthA}
    \noindent
    \parbox[t]{\TFlengthA}{\TrueFalse{#1}}\parbox[t]{\TFlengthB}{#2}
    \vspace{0.25in}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\theoremstyle{definition}
\newtheorem{problem}{Problem}
\newtheorem{theorem}{Theorem}
\newtheorem{defn}{Definition}
\newtheorem{refr}{References}
\newcommand{\E}{\mathbb E}
\newcommand{\R}{\mathbb R}
\DeclareMathOperator{\nnz}{nnz}
\DeclareMathOperator{\determinant}{det}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator{\sign}{sign}

\newcommand{\I}{\mathbf I}
\newcommand{\Q}{\mathbf Q}
\newcommand{\p}{\mathbf P}
\newcommand{\pb}{\bar {\p}}
\newcommand{\pbb}{\bar {\pb}}
\newcommand{\pr}{\bm \pi}

\newcommand{\trans}[1]{{#1}^{T}}
\newcommand{\loss}{\ell}
\newcommand{\w}{\mathbf w}
\newcommand{\x}{\mathbf x}
\newcommand{\y}{\mathbf y}
\newcommand{\lone}[1]{{\lVert {#1} \rVert}_1}
\newcommand{\ltwo}[1]{{\lVert {#1} \rVert}_2}
\newcommand{\lp}[1]{{\lVert {#1} \rVert}_p}
\newcommand{\linf}[1]{{\lVert {#1} \rVert}_\infty}
\newcommand{\lF}[1]{{\lVert {#1} \rVert}_F}

\newcommand{\Ein}{E_{\text{in}}}
\newcommand{\Eout}{E_{\text{out}}}
\newcommand{\Etest}{E_{\text{test}}}
\newcommand{\Eapp}{E_{\text{app}}}
\newcommand{\mH}{m_{\mathcal H}}
\newcommand{\dvc}{{d_{\text{VC}}}}
\newcommand{\HH}[1]{\mathcal H_{\text{#1}}}
\newcommand{\Hbinary}{\HH_{\text{binary}}}
\newcommand{\Haxis}{\HH_{\text{axis}}}
\newcommand{\Hperceptron}{\HH_{\text{perceptron}}}
\newcommand{\ignore}[1]{}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\printanswers

\begin{document}


\begin{center}
{
\Huge
    Midterm 2
}
\end{center}

\vspace{0.5in}
\noindent
\textbf{Printed Name:}

\noindent
\rule{\textwidth}{0.1pt}
\vspace{0.25in}

%Format:
%
    %The midterm will be worth 15 points.
    %There will be 10 True / False / Open questions. Each correct answer will be worth 1 point, each incorrect answer will result in a -1 penalty, and each blank answer will result in 0 points.
    %There will be 5 free response questions. These questions will follow the format of the questions in the notes packets.
%
    \noindent
\textbf{Due date:}
\begin{enumerate}
    \item
        The exam is due Sunday 6 Nov at midnight.

    \item
        You may submit it either on sakai electronically or by putting a physical copy under my door.
\end{enumerate}

\vspace{0.15in}
\noindent
\textbf{Rules:}
\begin{enumerate}
    \item
        The exam is untimed.
        You do not have to complete the exam in a single sitting.
        You may pause and restart whenever you'd like.

    \item
    You may use any non-human resources that you like, including notes, books, internet articles, and computers.

    \item
    You are not allowed to discuss the exam in any way with any human until after the due date. This includes:
        \begin{enumerate}
            \item obviously bad behavior like copying answers, 
            \item more banal behavior such as:
                \begin{enumerate}
                    \item telling your friend "Problem 6 was really hard" or 
                    \item asking your friend "Have you completed the exam yet?"
                \end{enumerate}
        \end{enumerate}
        
        Even after you finish the exam, you may not discuss it.

    \item
    If you do have questions about the exam, you should email me the questions rather than posting to github.

\end{enumerate}

\vspace{0.15in}
\noindent
\textbf{Grading:}
\begin{enumerate}
    \item For the True/False/Open questions: Each correct answer will be awarded +1 point, each incorrect answer will result in a -1 point penalty, and each blank answer will result in 0 points.

    \item All other problems are worth 1 point, with no penalty for incorrect answers.

    \item There are 16 points possible on the exam.
        Your final grade entered into sakai will be
        \begin{equation*}
            \min\{15, \text{the number of points earned}\}.
        \end{equation*}

    \item If you find a substantive error on the exam, then I will award you +1 bonus point.
\end{enumerate}

\vspace{0.15in}
\noindent
\textbf{Good luck :)}

\newpage

\begin{problem}
    For each statement below,
    circle \texttt{True} if the statement is known to be true,
    \texttt{False} if the statement is known to be false,
    and \texttt{Open} if the statement is not known to be either true or false.
    Ensure that you pay careful attention to the formal definitions of asymptotic notation in your responses.

\begin{enumerate}
    %\item\TFQuestion{F}{Let $f$ be the true label function.
        %Then it always the case that $\Eout(f) = 0$.}
    %\item\TFQuestion{T}{
        %}

    \item\TFQuestion{O}{For learning problems that are not linearly separable, the fastest possible algorithm for minimizing the 0-1 loss runs in exponential time in the number of feature dimensions $d$.}

    \item\TFQuestion{T}{Let $\mathcal H_1$ and $\mathcal H_2$ be two hypothesis classes satisfying $\dvc(\mathcal H_1) \le \dvc(\mathcal H_2)$.  Then $H_1 \subset H_2$.}

    \item\TFQuestion{T}{Let $\mathcal H$ be the perceptron hypothesis class with the number of feature dimensions $d=8$.  Then $\mH(6)=64$.}

    %\item\TFQuestion{T}{Let $\mathcal H_1$ be the perceptron hypothesis class with the PCA feature map applie
        
    \item\TFQuestion{T}{Let $\mathcal H$ be the perceptron hypothesis class and $\mathcal H_\Phi$ be the perceptron hypothesis class with the PCA kernel applied.
        Furthermore let $g\in\mathcal H$ and $g_\Phi\in\mathcal H_\Phi$ be the empirical risk minimizers.
        Then $\Ein(g) \le \Ein(g_\Phi)$.
        }
        
    \item\TFQuestion{T}{Let $\mathcal H$ be the perceptron hypothesis class and let $\mathcal H_\Phi$ be the perceptron hypothesis class with the decision stump feature map.
        Let $g\in\mathcal H$ and $g_\Phi \in \mathcal H_\Phi$ be the emperical risk minimizers.
        Then VC theory provides a better generalization bound for $g$ than for $g_\Phi$.}
        
    %\item\TFQuestion{T}{You have trained a logistic regression model $g$.  You evaluate it on your training set and test set and you notice that $|\Ein(g)-\Etest(g)|$ is much larger than $\Ein(g)$.  VC theory predicts that applying the PCA feature embedding with a low output dimension will reduce the generalization error.}

    \item\TFQuestion{F}{Define the approximation error of a hypothesis class $\mathcal{H}$ to be 
        \begin{equation}
            \Eapp = \min_{h\in\mathcal H} \Eout(h)
            .
        \end{equation}
        Then applying the random feature embedding with a low output degree will decrease the approximation error.}

    \item\TFQuestion{F}{Let
        $$
    \HH{axis2} = \bigg\{ \x \mapsto \sigma\sign(x_i) : \sigma \in\{+1, -1\}, i \in [d] \bigg\},
        $$
        and
        $$
    \HH{axis} = \bigg\{ \x \mapsto \sign(x_i) : i \in [d] \bigg\},
        $$
        Let $g_{\text{axis2}} \in \HH{axis2}$ and $g_{\text{axis}}\in\HH{axis}$ be the outputs of the TEA algorithm on their respective hypothesis classes.
        Then $\Ein(g_{\text{axis2}}) \le \Ein(g_{\text{axis}})$.
        }

    \item\TFQuestion{F}{
            Let $\mathcal H_1$ be the perceptron hypothesis class with the decision stump feature map applied,
            and $\mathcal H_2$ be the perceptron hypothesis class with the polynomial kernel of degree 3.
            Furthermore, let $g_1\in\mathcal H_1$ and $g_2\in\mathcal H_2$ be the emperical risk minimizers.
            Then VC theory predicts that $|\Eout(g_1)-\Etest(g_1)| \le |\Eout(g_2) - \Etest(g_2)|$ with high probability.
        }

    \item\TFQuestion{F}{
            Define the hypothesis class of positive and negative intervals in 1 dimension to be
            \begin{equation}
                \mathcal H = \bigg\{ x \mapsto \sigma \big\llbracket a \le x \le b \big\rrbracket : a \in \R, b \in \R, \sigma \in \{+1, -1\} \bigg\}.
            \end{equation}
            Then the $\dvc(\HH{}) = 3$.
        }

    %\item\TFQuestion{F}{
            %Let $\mathcal H$ be a hypothesis class.
            %If $\min_{h\in\mathcal H} \Ein(h) = 0$, then the VC dimension of $\mathcal H$ must be finite.
        %}

    \item\TFQuestion{F}{
            There does not exist a break point for the perceptron hypothesis class.
        }
\end{enumerate}
\end{problem}

\newpage
\begin{problem}
Either prove or give a counterexample to the following claim: 
Let $f$ be the true label function.
Then it must be the case that $\Ein(f)=0$.
\end{problem}
\begin{solution}
\end{solution}

\newpage
\begin{problem}
Either prove or give a counterexample to the following claim: 
Every surrogate loss function is convex.
%If $\mathcal H$ be a hypothesis class with $\dvc(\HH{})=4$.  Then $\mathcal H$ is guaranteed to be able to shatter all datasets of size $N=3$.
\end{problem}
\begin{solution}
\end{solution}

\newpage
\begin{problem}
What is the VC dimension of the following hypothesis class?
\begin{equation}
    \HH{} = \bigg\{ \x \mapsto \sigma\big\llbracket \ltwo{\x} \le \alpha \big\rrbracket : \sigma\in\{+1,-1\}, \alpha \in \R \bigg\}
\end{equation}
\end{problem}
\begin{solution}
\end{solution}

\newpage
\begin{problem}
    You are a bank using logistic regression to learn a formula for whether or not to issue a loan.
    Your dataset has $N$ data points and $d$ features,
    and you have trained a model $g$ using second order gradient descent so that your optimization error is negligible.
    You evaluate your model on the training and test sets and get values of $\Ein(g)=0.05$ and $\Etest(g)=0.41$.

    Your boss suggests that augmenting the dataset with more features might improve performance.
    Is this a good idea?  Use VC to justify why.
\end{problem}
\begin{solution}
\end{solution}

\newpage
\begin{problem}
You are training a logistic regression model with $N=10^{12}$ and $d=10^6$.
Which optimization algorithm do you choose and why?
\end{problem}
\begin{solution}
\end{solution}

\newpage
\begin{problem}
    You work at a car manufacturer and are developing a model to determine whether a part is defective or not.
    You are required by regulators to use support vector machines optimized with 2nd order gradient descent,
    but you are free to select any feature maps that you would like.
    Your training data has many features ($d=10^6$) but only a small number of data points ($N=10^3$).
    Which feature maps does VC theory predict would be a good choice?
\end{problem}
\begin{solution}
\end{solution}

\end{document}




