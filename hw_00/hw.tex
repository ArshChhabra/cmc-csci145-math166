\documentclass[10pt]{article}

\usepackage[margin=1in]{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{mathtools}

\usepackage{hyperref}
\hypersetup{
  colorlinks   = true, %Colours links instead of ugly boxes
  urlcolor     = black, %Colour for external hyperlinks
  linkcolor    = blue, %Colour of internal links
  citecolor    = blue  %Colour of citations
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\theoremstyle{definition}
\newtheorem{problem}{Problem}
\newcommand{\E}{\mathbb E}
\newcommand{\R}{\mathbb R}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

\newcommand{\trans}[1]{{#1}^{T}}
\newcommand{\loss}{\ell}
\newcommand{\w}{\mathbf w}
\newcommand{\x}{\mathbf x}
\newcommand{\y}{\mathbf y}
\newcommand{\ltwo}[1]{\lVert {#1} \rVert}

%Problem ideas:
%second derivative of the log-loss
%calculate the minimum of the OLS objective
%definition of strictly convex equivalent to second derivative positive semidefinite, first derivative condition
%jensen's inequality: arithemetic mean never less than geometric mean

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}


\begin{center}
    {
\Large
Homework 0: review of probability, statistics, linear algebra, and calculus
}

    \vspace{0.1in}
CSCI145/MATH166, Mike Izbicki

    \vspace{0.1in}
    \textbf{DUE: Thursday, 12 September at the beginning of class}

    \vspace{0.1in}
\end{center}

\vspace{0.25in}
\noindent
Name: 

\noindent
\rule{\textwidth}{0.1pt}
\vspace{0.25in}

\begin{problem}
    (10 pts)
    Section 1.2 of Bishop defines the following terms.
    Reproduce their definitions below.
    \begin{enumerate}
        \setlength\itemsep{8.0em}
        \item sum rule
        \item product rule
        \item Bayes Theorem
        \item expectation
        \item linearity of expectation 

            (not formally defined in Bishop, but is an immediate consequence of the definition of expectation)
            $$
            \text{if $x,y$ are random variables, then}~\E (x+y) = \E x + \E y
            $$
        \item law of large numbers (Eq. 1.35)
        \item conditional expectation
        \item variance 
        \item covariance 
        \item standard deviation
        %\item precision
        \item mode
            \\
            \\
            \\
            \\
            \\
    \end{enumerate}

    I will use the following terms frequently in class, 
    so you need to know what they mean,
    but you do not need to be able to formally define them:
    \emph{marginal distribution, conditional distribution, joint distribution, prior distribution, posterior distribution, normalization, probability density function (PDF), cumulative distribution function (CDF), probability mass function (PMF), independent and identically distributed, bias, overfitting, frequentist statistics versus bayesian statistics, likelihood function, error function, maximum likelihood (ML), maximum a posteriori (MAP), hyperparameters, measure theory}.

\end{problem}

%\newpage
%\begin{problem}
    %Write the probability density functions of 
    %\begin{enumerate}
        %\setlength\itemsep{3in}
        %\item normal distribution
        %\item 
    %\end{enumerate}
%\end{problem}

\newpage
\begin{problem}
    (15 pts)
    Prove or give a counter example:
    If two random variables $x$,$y$ are independent, then their covariance is zero.
\end{problem}

\newpage
\begin{problem}
    (15 pts)
    If two random variables $x$,$y$ have zero covariance, then they are independent.
\end{problem}

\newpage
\begin{problem}
    (10pts)
    The variance can be defined as either
    \begin{equation}
        \Var(x) = \E(x - \E(x))^2
    \end{equation}
    or 
    \begin{equation}
        \Var(x) = \E (x^2) - (\E x)^2
        .
    \end{equation}
    Show that these two definitions are equivalent.
    HINT:
    The problem requires applying the definition of expectation and algebraic manipulations.
\end{problem}

\newpage
\begin{problem}
    (5 pts)
    Section 9.5 of \emph{The Matrix Cookbook} defines orthogonal matrices and lists some of their properties.
    Reproduce this definition and properties below.
    (You do not need to reproduce subsections 9.5.1, 9.5.2, or 9.5.3, just the main information under 9.5.)
    %\begin{enumerate}
        %\setlength\itemsep{0.5in}
        %\item positive (semi)definite
        %\item sparsity
        %\item rank
        %\item condition number %of a matrix in terms of its eigenvalues (Eq. 154)
        %%\item the vector $L_1$ norm
        %%\item the vector $L_2$ norm
        %%\item the vector $L_p$ norm
        %%\item the vector $L_\infty$ norm
            %\\
            %\\
            %\\
    %\end{enumerate}
%\textbf{NOTE:} You will need to know all the facts about positive (semi-)definite matrices in Section 9.6 of the matrix cookbook.
\end{problem}

\newpage
\begin{problem}
    (5 pts)
    Sections 9.6.1-9.6.5 of \emph{The Matrix Cookbook} defines positive (semi-)definite matrices and lists some of their properties.
    Reproduce this definition and properties below.
\end{problem}

\newpage
\begin{problem}
    (10 pts)
    A vector/matrix \emph{norm} must satisfy four requirements 
    (Equations 529-532 of \emph{The Matrix Cookbook}).
    State them.
    \begin{enumerate}
        \setlength\itemsep{0.5in}
        \item
        \item
        \item
        \item ~
            \\
            \\
            \\
    \end{enumerate}
    Define the $L_1$ vector norm (Eq. 525), $L_2$ vector norm (Eq. 526), induced/operator norm of a matrix, the $L_1$ matrix norm (Eq. 537), the $L_2$ matrix norm (Eq. 538), and the Frobenius norm (Eq. 541).
\end{problem}

\newpage
\begin{problem}
    (15 pts)
    The log-loss is defined to be
    \begin{equation}
        \label{eq:ll}
        \loss(\x;\w) = \log(1+\exp(-\trans\x\w)) 
        .
    \end{equation}
    Calculate the first and second derivatives of \eqref{eq:ll} with respect to $\w$.
    Note that because $\w$ is a vector, the loss is a scalar, the derivative is a vector, and the second derivative is a matrix.
    See Section 2 of \emph{The Matrix Cookbook} for a review of vector derivatives.
\end{problem}

\newpage
\begin{problem}
    (15 pts)
    Calculate 
    \begin{equation}
        \argmin_{\w} \bigg(\ltwo{\y - X\w}^2 + \lambda\ltwo{\w}^2\bigg)
    \end{equation}
    where $\w$ is a vector of dimension $d$,
    $\y$ is a vector of dimension $n$, 
    and $X$ is a matrix with shape $n\times d$.

    HINT: Recall that the $\argmin$ is defined as
    \begin{equation}
        \argmin_\w f(\w) \triangleq \{ \w : f(\w) = \min_{\w'} f(\w') \}
        .
    \end{equation}
    That is, the $\argmin$ returns the set of values that minimize the function $f$.
    To calculate the $\argmin$, take the derivative of $f$ with respect to $\w$, 
    set it equal to zero, 
    and solve for $\w$.
    %Note that because $\w$ is a vector, the loss is a scalar, the derivative is a vector, and the second derivative is a matrix.
    %See Section 2 of \emph{The Matrix Cookbook} for a review of vector derivatives.
\end{problem}
\vspace{6.5in}
\begin{problem}
    (2pts extra credit)
    If you complete this assignment in \LaTeX, then you will receive +2 pts extra credit.
    The source files are available at: 
    
    \url{https://github.com/mikeizbicki/cmc-csci145-math166/tree/master/hw_00}.
\end{problem}
\end{document}

