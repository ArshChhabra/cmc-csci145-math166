\documentclass[10pt]{article}

\usepackage[margin=1in]{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{mathtools}

\usepackage{hyperref}
\hypersetup{
  colorlinks   = true, %Colours links instead of ugly boxes
  urlcolor     = black, %Colour for external hyperlinks
  linkcolor    = blue, %Colour of internal links
  citecolor    = blue  %Colour of citations
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\theoremstyle{definition}
\newtheorem{problem}{Problem}
\newcommand{\E}{\mathbb E}
\newcommand{\R}{\mathbb R}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\Cov}{Cov}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

\newcommand{\trans}[1]{{#1}^{T}}
\newcommand{\loss}{\ell}
\newcommand{\w}{\mathbf w}
\newcommand{\x}{\mathbf x}
\newcommand{\y}{\mathbf y}
\newcommand{\ltwo}[1]{\lVert {#1} \rVert}

%Problem ideas:
%second derivative of the log-loss
%calculate the minimum of the OLS objective
%definition of strictly convex equivalent to second derivative positive semidefinite, first derivative condition
%jensen's inequality: arithemetic mean never less than geometric mean

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}


\begin{center}
    {
\Large
Solutions

    \vspace{0.2in}
Homework 0: review of probability, statistics, linear algebra, and calculus
}

    \vspace{0.1in}
CSCI145/MATH166, Mike Izbicki

    \vspace{0.1in}
\end{center}

\begin{problem}
    (10 pts)
    A couple people missed points on this problem for not defining the terms.
\end{problem}

\begin{problem}
    (15 pts)
    Prove or give a counter example:
    If two random variables $x$,$y$ are independent, then their covariance is zero.
\end{problem}
\begin{proof}
    First, we show that %$\E_{x,y} xy = (\E_x x)(\E_y y)$.
    \begin{align}
        \E_{x,y} xy 
        &= \iint xy~p(x,y)~dx~dy \\
        &= \iint xy~p(x)~p(y)~dx~dy \\
        &= \bigg(\int x~p(x)~dx\bigg) \bigg(\int y~p(y)~dy \bigg) \\
        &= (\E_x x)(\E_y y)
        \label{eq:pt1}
    \end{align}
    Then by the definition of covariance,
    \begin{align}
        \Cov(x,y) = \E_{x,y} xy - (\E_x x)(\E_y y) = 0.
    \end{align}
\end{proof}

\noindent
\textit{Grading notes:} 
I was originally expecting you to explicitly prove equation \eqref{eq:pt1},
but decided to give you full credit even if you just assumed it.

\begin{problem}
    (15 pts)
    Prove or give a counter example:
    If two random variables $x$,$y$ have zero covariance, then they are independent.
\end{problem}

\noindent
\textit{Example wrong (but close) answer:}
This statement is false.
As a counterexample, let $x$ be a random variable with $\E x = 0$ and $\E x^3 = 0$,
and let $y=x^2$.
Then $x$ and $y$ are not independent, but
\begin{align}
    \Cov(x,y)
    &= \E xy - \E x \E y \\
    &= \E x^3 - \E x \E y \\
    &= 0.
\end{align}

\noindent
\textit{Why is this wrong?}
The definition of $x$ and $y$ do not imply that $x$ and $y$ are dependent.
For example, let $x$ be the random variable that takes on values of $1$ or $-1$ each with probability $1/2$.
Then $x$ satisfies the assumptions above (i.e.\ $\E x = 0$ and $\E x^3 = 0$).
No matter what the value of $x$ is, $y$ will always be equal to 1.
And so in this case, $x$ and $y$ are independent.
An even simpler example is if $x$ and $y$ are both the constant zero random variables.

\vspace{0.1in}
\noindent
\textit{How to fix it?}
Whenever you provide a counterexample, it is not enough to describe the counterexample.
You must explicitly state what the example is.
In this case, if we let $x$ be the uniform distribution over $[-1,1]$,
then $x$ and $y$ are actually dependent on each other,
and the remainder of the argument holds.

\begin{problem}
    (10pts)
    Everyone got this mostly correct.
\end{problem}

\begin{problem}
    (5 pts)
    Everyone got full credit.
\end{problem}

\begin{problem}
    (5 pts)
    Everyone got full credit.
\end{problem}

\begin{problem}
    (10 pts)
    Everyone got full credit.
\end{problem}

\begin{problem}
    (15 pts)
    The log-loss is defined to be
    \begin{equation}
        \label{eq:ll}
        \loss(\x;\w) = \log(1+\exp(-\trans\x\w)) 
        .
    \end{equation}
    Calculate the first and second derivatives of \eqref{eq:ll} with respect to $\w$.
    Note that because $\w$ is a vector, the loss is a scalar, the derivative is a vector, and the second derivative is a matrix.
    See Section 2 of \emph{The Matrix Cookbook} for a review of vector derivatives.
\end{problem}

\noindent
The first derivative is:
\begin{align}
    \frac{d}{d\w} \loss(\x;\w)
    &=
    \frac{d}{d\w} \log(1+\exp(-\trans\x\w)) \\
    &=
    \frac{1}{1+\exp(-\trans\x\w))}\frac{d}{d\w}(1+\exp(-\trans\x\w)) \\
    &=
    \frac{-\x\exp(-\trans\x\w)}{1+\exp(-\trans\x\w))} \\
    &=
    \frac{-\x}{1+\exp(\trans\x\w))}
\end{align}
The second derivative is:
\begin{align}
    \frac{d}{d\w} \frac{d}{d\w} \loss(\x;\w)
    &=
    \frac{d}{d\w} -\x(1+\exp(\trans\x\w)))^{-1} \\
    &=
    \frac{d}{d\w} -\x(1+\exp(\trans\x\w)))^{-1} \\
    %&=
    %\frac{d(1+\exp(\trans\x\w))}{d\w} -\x(1+\exp(\trans\x\w)))^{-1} \\
    &=
    \bigg[\frac{d}{d\w} (1+\exp(\trans\x\w)) \bigg] \bigg[\trans\x(1+\exp(\trans\x\w)))^{-2} \bigg]\\
    &=
    \bigg[\x (\exp(\trans\x\w)) \bigg] \bigg[\trans\x(1+\exp(\trans\x\w)))^{-2} \bigg]\\
    &=
    \x\trans\x (\exp(\trans\x\w))(1+\exp(\trans\x\w)))^{-2}
\end{align}

\noindent
\textit{Grading notes:}
\begin{enumerate}
    \item if you had the type of the derivatives being scalars (instead of vectors/matrices), you lost 5 pts each
    \item You cannot square a vector, i.e.\ $x^2$ does not make sense.
        Similarly, you cannot multiply a vector times itself; $\x\x$ and $\trans\x\trans\x$ don't make sense.
        -3 for this mistake.
\end{enumerate}

\newpage
\begin{problem}
    (15 pts)
    Calculate 
    \begin{equation}
        \argmin_{\w} \bigg(\ltwo{\y - X\w}^2 + \lambda\ltwo{\w}^2\bigg)
    \end{equation}
    where $\w$ is a vector of dimension $d$,
    $\y$ is a vector of dimension $n$, 
    and $X$ is a matrix with shape $n\times d$.

    HINT: Recall that the $\argmin$ is defined as
    \begin{equation}
        \argmin_\w f(\w) \triangleq \{ \w : f(\w) = \min_{\w'} f(\w') \}
        .
    \end{equation}
    That is, the $\argmin$ returns the set of values that minimize the function $f$.
    To calculate the $\argmin$, take the derivative of $f$ with respect to $\w$, 
    set it equal to zero, 
    and solve for $\w$.
    %Note that because $\w$ is a vector, the loss is a scalar, the derivative is a vector, and the second derivative is a matrix.
    %See Section 2 of \emph{The Matrix Cookbook} for a review of vector derivatives.
\end{problem}

\begin{align}
    0
    &= \frac{d}{d\w} \bigg[ \ltwo{\y-X\w}^2 + \lambda\ltwo{\w}^2 \bigg] \\
    &= \frac{d}{d\w} \ltwo{\y-X\w}^2 + \frac{d}{d\w}\lambda\ltwo{\w}^2 \\
    &= -2\trans X(\y-X\w) + 2\lambda\w \\
    &= -\trans X\y+\trans X X\w + \lambda\w \\
    \trans X\y &= \trans X X\w + \lambda\w \\
    &= (\trans X X + \lambda I) \w \\
    (\trans X X + \lambda I)^{-1}\trans X\y &= \w
\end{align}

\noindent
\textit{Grading notes:}
\begin{enumerate}
    \item 
        Let $A$ be a square matrix and $\lambda$ a scalar.  
        $A+\lambda$ is different than $A+\lambda I$.
        The former adds the constant to every element, the latter only along the diagonal.
        -2 points for leaving out the $I$.

    \item
        You cannot divide by a matrix, you must multiply by the inverse.
        This is because matrix multiplication is not commutative, so you must specify whether it's a left or right multiplication.
        -2 for this mistake.
\end{enumerate}

\begin{problem}
    (2pts extra credit)
    If you complete this assignment in \LaTeX, then you will receive +2 pts extra credit.
    The source files are available at: 
    
    \url{https://github.com/mikeizbicki/cmc-csci145-math166/tree/master/hw_00}.
\end{problem}

\noindent
\textit{Grading notes:}
\begin{enumerate}
    \item
In the future, you will need to have better typesetting to get the \LaTeX\ extra credit.
\item
    You cannot put text directly in math mode.  For example,
        \begin{equation}
            log(1+exp(a))
        \end{equation}
        is bad.  
        Instead, do
        \begin{equation}
            \log(1+\exp(a))
        \end{equation}
        by typing \texttt{\textbackslash log} and \texttt{\textbackslash exp} instead of just \texttt{log} and \texttt{exp}.
    \item
        Similarly, you should use $\E$ not $E$, $\max$ not $max$, etc.
    \item
        We do not use \texttt{*} for multiplication ever.  Only juxtaposition or $\cdot$ (\texttt{\textbackslash cdot}).
    \item 
        If you have a multiline equation, then you must use the \texttt{align} environment and have the $=$ properly aligned.
    \item 
        I don't care whether you use $x$ or $X$ type variables, but you must be consistent.
\end{enumerate}
\end{document}

